{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d48a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f494a",
   "metadata": {},
   "source": [
    "# Load Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193a135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = \"\"\"Britain's communications intelligence agency GCHQ has issued a statement\n",
    "denying it wiretapped Donald Trump during the US presidential campaign. The\n",
    "unusual move by the agency came after White House Press Secretary Sean\n",
    "Spicer cited claims first made on US TV channel Fox News earlier this week.\n",
    "GCHQ responded by saying the allegations were \"nonsense, utterly ridiculous\n",
    "and should be ignored\". The claims Of GCHQ involvement Were initially made\n",
    "by former judge Andrew Napolitano. Mr Spicer quoted Mr Napolitano as\n",
    "saying: \"Three intelligence Sources have informed Fox News that president\n",
    "Obama went outside the chain of command. \"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809392b4",
   "metadata": {},
   "source": [
    "# Transform to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43e917ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'britain\\'s communications intelligence agency gchq has issued a statement\\ndenying it wiretapped donald trump during the us presidential campaign. the\\nunusual move by the agency came after white house press secretary sean\\nspicer cited claims first made on us tv channel fox news earlier this week.\\ngchq responded by saying the allegations were \"nonsense, utterly ridiculous\\nand should be ignored\". the claims of gchq involvement were initially made\\nby former judge andrew napolitano. mr spicer quoted mr napolitano as\\nsaying: \"three intelligence sources have informed fox news that president\\nobama went outside the chain of command. \"\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_txt = sample_txt.lower()\n",
    "sample_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c843b9df",
   "metadata": {},
   "source": [
    "# Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb7474d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'britain\\'s communications intelligence agency gchq has issued a statement\\ndenying it wiretapped donald trump during the us presidential campaign. the\\nunusual move by the agency came after white house press secretary sean\\nspicer cited claims first made on us tv channel fox news earlier this week.\\ngchq responded by saying the allegations were \"nonsense, utterly ridiculous\\nand should be ignored\". the claims of gchq involvement were initially made\\nby former judge andrew napolitano. mr spicer quoted mr napolitano as\\nsaying: \"three intelligence sources have informed fox news that president\\nobama went outside the chain of command. \"\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sample_txt = re.sub(r'\\d+', '', sample_txt)\n",
    "sample_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260c97a",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ab885",
   "metadata": {},
   "source": [
    "## Sentence Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f138860b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "britain's communications intelligence agency gchq has issued a statement\n",
      "denying it wiretapped donald trump during the us presidential campaign.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "sample_sentences = sent_tokenize(sample_txt)\n",
    "print(sample_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d0b921",
   "metadata": {},
   "source": [
    "## Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af5d65a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['britain', \"'s\", 'communications', 'intelligence', 'agency', 'gchq', 'has', 'issued', 'a', 'statement', 'denying', 'it', 'wiretapped', 'donald', 'trump', 'during', 'the', 'us', 'presidential', 'campaign', '.', 'the', 'unusual', 'move', 'by', 'the', 'agency', 'came', 'after', 'white', 'house', 'press', 'secretary', 'sean', 'spicer', 'cited', 'claims', 'first', 'made', 'on', 'us', 'tv', 'channel', 'fox', 'news', 'earlier', 'this', 'week', '.', 'gchq', 'responded', 'by', 'saying', 'the', 'allegations', 'were', '``', 'nonsense', ',', 'utterly', 'ridiculous', 'and', 'should', 'be', 'ignored', \"''\", '.', 'the', 'claims', 'of', 'gchq', 'involvement', 'were', 'initially', 'made', 'by', 'former', 'judge', 'andrew', 'napolitano', '.', 'mr', 'spicer', 'quoted', 'mr', 'napolitano', 'as', 'saying', ':', '``', 'three', 'intelligence', 'sources', 'have', 'informed', 'fox', 'news', 'that', 'president', 'obama', 'went', 'outside', 'the', 'chain', 'of', 'command.', '``']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "sample_words = word_tokenize(sample_txt)\n",
    "print(sample_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c6a77",
   "metadata": {},
   "source": [
    "# Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b03aee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a07615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['britain', 's', 'communications', 'intelligence', 'agency', 'gchq', 'has', 'issued', 'a', 'statement', 'denying', 'it', 'wiretapped', 'donald', 'trump', 'during', 'the', 'us', 'presidential', 'campaign', '', 'the', 'unusual', 'move', 'by', 'the', 'agency', 'came', 'after', 'white', 'house', 'press', 'secretary', 'sean', 'spicer', 'cited', 'claims', 'first', 'made', 'on', 'us', 'tv', 'channel', 'fox', 'news', 'earlier', 'this', 'week', '', 'gchq', 'responded', 'by', 'saying', 'the', 'allegations', 'were', '', 'nonsense', '', 'utterly', 'ridiculous', 'and', 'should', 'be', 'ignored', '', '', 'the', 'claims', 'of', 'gchq', 'involvement', 'were', 'initially', 'made', 'by', 'former', 'judge', 'andrew', 'napolitano', '', 'mr', 'spicer', 'quoted', 'mr', 'napolitano', 'as', 'saying', '', '', 'three', 'intelligence', 'sources', 'have', 'informed', 'fox', 'news', 'that', 'president', 'obama', 'went', 'outside', 'the', 'chain', 'of', 'command', '']\n"
     ]
    }
   ],
   "source": [
    "table = str.maketrans('', '', string.punctuation)\n",
    "sample_words = [str(w).translate(table) for w in sample_words]\n",
    "print(sample_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47efa4d3",
   "metadata": {},
   "source": [
    "# Filter Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0e68bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<WordListCorpusReader in 'C:\\\\Users\\\\Abdullah\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf79e546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['britain', 'communications', 'intelligence', 'agency', 'gchq', 'issued', 'statement', 'denying', 'wiretapped', 'donald', 'trump', 'us', 'presidential', 'campaign', '', 'unusual', 'move', 'agency', 'came', 'white', 'house', 'press', 'secretary', 'sean', 'spicer', 'cited', 'claims', 'first', 'made', 'us', 'tv', 'channel', 'fox', 'news', 'earlier', 'week', '', 'gchq', 'responded', 'saying', 'allegations', '', 'nonsense', '', 'utterly', 'ridiculous', 'ignored', '', '', 'claims', 'gchq', 'involvement', 'initially', 'made', 'former', 'judge', 'andrew', 'napolitano', '', 'mr', 'spicer', 'quoted', 'mr', 'napolitano', 'saying', '', '', 'three', 'intelligence', 'sources', 'informed', 'fox', 'news', 'president', 'obama', 'went', 'outside', 'chain', 'command', '']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "sample_words = [w for w in sample_words if not w in stop_words]\n",
    "print(sample_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d3fcb",
   "metadata": {},
   "source": [
    "# Stem Words - Root Form (Might have no meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46ba99d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['britain', 'commun', 'intellig', 'agenc', 'gchq', 'issu', 'statement', 'deni', 'wiretap', 'donald', 'trump', 'us', 'presidenti', 'campaign', '', 'unusu', 'move', 'agenc', 'came', 'white', 'hous', 'press', 'secretari', 'sean', 'spicer', 'cite', 'claim', 'first', 'made', 'us', 'tv', 'channel', 'fox', 'news', 'earlier', 'week', '', 'gchq', 'respond', 'say', 'alleg', '', 'nonsens', '', 'utterli', 'ridicul', 'ignor', '', '', 'claim', 'gchq', 'involv', 'initi', 'made', 'former', 'judg', 'andrew', 'napolitano', '', 'mr', 'spicer', 'quot', 'mr', 'napolitano', 'say', '', '', 'three', 'intellig', 'sourc', 'inform', 'fox', 'news', 'presid', 'obama', 'went', 'outsid', 'chain', 'command', '']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stem_words = [porter.stem(word) for word in sample_words]\n",
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9daf6",
   "metadata": {},
   "source": [
    "# Lemmatization (Returns proper words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "213a1f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "britain\n",
      "communication\n",
      "intelligence\n",
      "agency\n",
      "gchq\n",
      "issued\n",
      "statement\n",
      "denying\n",
      "wiretapped\n",
      "donald\n",
      "trump\n",
      "u\n",
      "presidential\n",
      "campaign\n",
      "\n",
      "unusual\n",
      "move\n",
      "agency\n",
      "came\n",
      "white\n",
      "house\n",
      "press\n",
      "secretary\n",
      "sean\n",
      "spicer\n",
      "cited\n",
      "claim\n",
      "first\n",
      "made\n",
      "u\n",
      "tv\n",
      "channel\n",
      "fox\n",
      "news\n",
      "earlier\n",
      "week\n",
      "\n",
      "gchq\n",
      "responded\n",
      "saying\n",
      "allegation\n",
      "\n",
      "nonsense\n",
      "\n",
      "utterly\n",
      "ridiculous\n",
      "ignored\n",
      "\n",
      "\n",
      "claim\n",
      "gchq\n",
      "involvement\n",
      "initially\n",
      "made\n",
      "former\n",
      "judge\n",
      "andrew\n",
      "napolitano\n",
      "\n",
      "mr\n",
      "spicer\n",
      "quoted\n",
      "mr\n",
      "napolitano\n",
      "saying\n",
      "\n",
      "\n",
      "three\n",
      "intelligence\n",
      "source\n",
      "informed\n",
      "fox\n",
      "news\n",
      "president\n",
      "obama\n",
      "went\n",
      "outside\n",
      "chain\n",
      "command\n",
      "\n",
      "['britain', ' ', 'communication', ' ', 'intelligence', ' ', 'agency', ' ', 'gchq', ' ', 'issued', ' ', 'statement', ' ', 'denying', ' ', 'wiretapped', ' ', 'donald', ' ', 'trump', ' ', 'u', ' ', 'presidential', ' ', 'campaign', ' ', '', ' ', 'unusual', ' ', 'move', ' ', 'agency', ' ', 'came', ' ', 'white', ' ', 'house', ' ', 'press', ' ', 'secretary', ' ', 'sean', ' ', 'spicer', ' ', 'cited', ' ', 'claim', ' ', 'first', ' ', 'made', ' ', 'u', ' ', 'tv', ' ', 'channel', ' ', 'fox', ' ', 'news', ' ', 'earlier', ' ', 'week', ' ', '', ' ', 'gchq', ' ', 'responded', ' ', 'saying', ' ', 'allegation', ' ', '', ' ', 'nonsense', ' ', '', ' ', 'utterly', ' ', 'ridiculous', ' ', 'ignored', ' ', '', ' ', '', ' ', 'claim', ' ', 'gchq', ' ', 'involvement', ' ', 'initially', ' ', 'made', ' ', 'former', ' ', 'judge', ' ', 'andrew', ' ', 'napolitano', ' ', '', ' ', 'mr', ' ', 'spicer', ' ', 'quoted', ' ', 'mr', ' ', 'napolitano', ' ', 'saying', ' ', '', ' ', '', ' ', 'three', ' ', 'intelligence', ' ', 'source', ' ', 'informed', ' ', 'fox', ' ', 'news', ' ', 'president', ' ', 'obama', ' ', 'went', ' ', 'outside', ' ', 'chain', ' ', 'command', ' ', '', ' ']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemm_words = []\n",
    "for word in sample_words:\n",
    "    print(lemmatizer.lemmatize(word))\n",
    "    lemm_words.append(lemmatizer.lemmatize(word))\n",
    "    lemm_words.append(\" \")\n",
    "    \n",
    "print(lemm_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed01f10a",
   "metadata": {},
   "source": [
    "# Bag of Words Model (Stand-alone words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e966b2d",
   "metadata": {},
   "source": [
    "## Count Vectorizer (Does not give more weightage to imp words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49d474c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'after': 0, 'agency': 1, 'allegations': 2, 'and': 3, 'andrew': 4, 'as': 5, 'be': 6, 'britain': 7, 'by': 8, 'came': 9, 'campaign': 10, 'chain': 11, 'channel': 12, 'cited': 13, 'claims': 14, 'command': 15, 'communications': 16, 'denying': 17, 'donald': 18, 'during': 19, 'earlier': 20, 'first': 21, 'former': 22, 'fox': 23, 'gchq': 24, 'has': 25, 'have': 26, 'house': 27, 'ignored': 28, 'informed': 29, 'initially': 30, 'intelligence': 31, 'involvement': 32, 'issued': 33, 'it': 34, 'judge': 35, 'made': 36, 'move': 37, 'mr': 38, 'napolitano': 39, 'news': 40, 'nonsense': 41, 'obama': 42, 'of': 43, 'on': 44, 'outside': 45, 'president': 46, 'presidential': 47, 'press': 48, 'quoted': 49, 'responded': 50, 'ridiculous': 51, 'saying': 52, 'sean': 53, 'secretary': 54, 'should': 55, 'sources': 56, 'spicer': 57, 'statement': 58, 'that': 59, 'the': 60, 'this': 61, 'three': 62, 'trump': 63, 'tv': 64, 'unusual': 65, 'us': 66, 'utterly': 67, 'week': 68, 'went': 69, 'were': 70, 'white': 71, 'wiretapped': 72}\n",
      "(5, 73)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0\n",
      "  1]\n",
      " [1 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  1 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 2 1 0 0 1 1 1 0 1 0 0 1\n",
      "  0]\n",
      " [0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0\n",
      "  0]\n",
      " [0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1\n",
      "  1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0\n",
      "  0 0 2 1 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0\n",
      "  0]]\n"
     ]
    }
   ],
   "source": [
    "# Takes care of removing stopwords, lemmatization etc.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(sample_sentences)\n",
    "\n",
    "#print(vectorizer.vocabulary_)\n",
    "print({k: v for k, v in sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])}) # Sorted\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "vector = vectorizer.transform(sample_sentences)\n",
    "\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39709c3c",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0d47838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 2 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0]]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = [\"\"\"There are rumours that Sean Spicer has been involved in utterly ridiculous activities - Bill Spicer\"\"\"]\n",
    "\n",
    "# Vectorizer already fitted on our previous sample text\n",
    "vector = vectorizer.transform(test_sentence)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab6543",
   "metadata": {},
   "source": [
    "# TF - IDF Vectorizer (Freq of word in sentence X freq in other sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d97a0",
   "metadata": {},
   "source": [
    "## *Best way*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fac4a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = [\"\"\"There are rumours that Sean Spicer has been involved in utterly ridiculous activities - Bill Spicer\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df623f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'after': 0, 'agency': 1, 'allegations': 2, 'and': 3, 'andrew': 4, 'as': 5, 'be': 6, 'britain': 7, 'by': 8, 'came': 9, 'campaign': 10, 'chain': 11, 'channel': 12, 'cited': 13, 'claims': 14, 'command': 15, 'communications': 16, 'denying': 17, 'donald': 18, 'during': 19, 'earlier': 20, 'first': 21, 'former': 22, 'fox': 23, 'gchq': 24, 'has': 25, 'have': 26, 'house': 27, 'ignored': 28, 'informed': 29, 'initially': 30, 'intelligence': 31, 'involvement': 32, 'issued': 33, 'it': 34, 'judge': 35, 'made': 36, 'move': 37, 'mr': 38, 'napolitano': 39, 'news': 40, 'nonsense': 41, 'obama': 42, 'of': 43, 'on': 44, 'outside': 45, 'president': 46, 'presidential': 47, 'press': 48, 'quoted': 49, 'responded': 50, 'ridiculous': 51, 'saying': 52, 'sean': 53, 'secretary': 54, 'should': 55, 'sources': 56, 'spicer': 57, 'statement': 58, 'that': 59, 'the': 60, 'this': 61, 'three': 62, 'trump': 63, 'tv': 64, 'unusual': 65, 'us': 66, 'utterly': 67, 'week': 68, 'went': 69, 'were': 70, 'white': 71, 'wiretapped': 72}\n",
      "\n",
      "\n",
      "[2.09861229 1.69314718 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 1.40546511 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 1.69314718 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229 1.69314718\n",
      " 1.40546511 2.09861229 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 1.69314718 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 1.69314718 2.09861229 2.09861229 1.69314718 1.69314718 2.09861229\n",
      " 2.09861229 1.69314718 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 2.09861229 1.69314718 2.09861229\n",
      " 2.09861229 2.09861229 2.09861229 1.69314718 2.09861229 2.09861229\n",
      " 1.         2.09861229 2.09861229 2.09861229 2.09861229 2.09861229\n",
      " 1.69314718 2.09861229 2.09861229 2.09861229 1.69314718 2.09861229\n",
      " 2.09861229]\n",
      "Vector shape \n",
      " (1, 73)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.36265071 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.36265071 0.         0.36265071\n",
      "  0.         0.         0.         0.58516862 0.         0.36265071\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.36265071 0.         0.         0.         0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vectorizer.fit(sample_sentences)\n",
    "\n",
    "#print(vectorizer.vocabulary_)\n",
    "print({k: v for k, v in sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])}) # Sorted\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(vectorizer.idf_)\n",
    "\n",
    "vector = vectorizer.transform(test_sentence)\n",
    "\n",
    "print(\"Vector shape \\n\", vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60631df3",
   "metadata": {},
   "source": [
    "# Hash Vectorizer (Two words might be hashed to same number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "744eec74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 50)\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -0.25        0.25       -0.25        0.\n",
      "   0.25        0.          0.          0.          0.          0.25\n",
      "   0.25        0.          0.          0.25        0.          0.25\n",
      "   0.          0.          0.          0.          0.         -0.25\n",
      "   0.          0.          0.25       -0.25        0.          0.\n",
      "   0.          0.25        0.         -0.25        0.          0.\n",
      "  -0.25        0.25        0.          0.         -0.25        0.\n",
      "   0.          0.        ]\n",
      " [ 0.          0.          0.          0.         -0.20851441 -0.20851441\n",
      "   0.          0.          0.         -0.20851441  0.20851441 -0.41702883\n",
      "  -0.20851441  0.          0.20851441 -0.20851441  0.          0.\n",
      "   0.          0.         -0.20851441  0.          0.          0.\n",
      "   0.         -0.20851441  0.          0.         -0.20851441  0.\n",
      "  -0.20851441 -0.20851441  0.20851441  0.          0.         -0.20851441\n",
      "   0.          0.          0.          0.          0.          0.41702883\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.20851441]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.28867513  0.          0.          0.          0.\n",
      "   0.28867513  0.          0.          0.          0.28867513  0.\n",
      "   0.28867513  0.          0.          0.          0.          0.28867513\n",
      "   0.          0.          0.          0.          0.28867513  0.\n",
      "   0.          0.         -0.28867513  0.28867513  0.          0.\n",
      "  -0.28867513  0.28867513  0.          0.          0.          0.\n",
      "   0.          0.          0.         -0.28867513  0.          0.\n",
      "   0.          0.28867513]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.         -0.2773501  -0.5547002   0.          0.\n",
      "   0.          0.          0.          0.         -0.2773501   0.\n",
      "   0.          0.          0.          0.          0.          0.2773501\n",
      "  -0.2773501   0.         -0.2773501   0.          0.          0.\n",
      "  -0.2773501   0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.2773501   0.          0.\n",
      "   0.          0.          0.          0.          0.2773501   0.\n",
      "   0.          0.2773501 ]\n",
      " [ 0.          0.          0.          0.          0.24253563  0.\n",
      "   0.24253563  0.          0.          0.          0.24253563 -0.24253563\n",
      "   0.         -0.48507125  0.          0.24253563 -0.24253563  0.\n",
      "   0.         -0.24253563  0.          0.24253563  0.          0.\n",
      "   0.          0.          0.24253563  0.         -0.24253563  0.\n",
      "   0.          0.          0.          0.         -0.24253563  0.24253563\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.24253563  0.\n",
      "   0.          0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HashingVectorizer(n_features=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HashingVectorizer</label><div class=\"sk-toggleable__content\"><pre>HashingVectorizer(n_features=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "HashingVectorizer(n_features=50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=50)\n",
    "\n",
    "vector = vectorizer.transform(sample_sentences)\n",
    "\n",
    "print(vector.shape)\n",
    "print(vector.toarray())\n",
    "\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcac9bf",
   "metadata": {},
   "source": [
    "# Model using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e573d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "556f3192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_df = pd.read_csv(\"spam.csv\")\n",
    "data_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0164431",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = data_df[['text']].values\n",
    "data_y = data_df[['label']].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71cbf29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = [x[0].strip() for x in X_train.tolist()]\n",
    "test_X = [x[0].strip() for x in X_test.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b9f3409",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "    ('clf', LogisticRegression())])\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "108e8f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=2,\n",
       "             estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                        TfidfVectorizer(stop_words={&#x27;a&#x27;,\n",
       "                                                                    &#x27;about&#x27;,\n",
       "                                                                    &#x27;above&#x27;,\n",
       "                                                                    &#x27;after&#x27;,\n",
       "                                                                    &#x27;again&#x27;,\n",
       "                                                                    &#x27;against&#x27;,\n",
       "                                                                    &#x27;ain&#x27;,\n",
       "                                                                    &#x27;all&#x27;, &#x27;am&#x27;,\n",
       "                                                                    &#x27;an&#x27;, &#x27;and&#x27;,\n",
       "                                                                    &#x27;any&#x27;,\n",
       "                                                                    &#x27;are&#x27;,\n",
       "                                                                    &#x27;aren&#x27;,\n",
       "                                                                    &quot;aren&#x27;t&quot;,\n",
       "                                                                    &#x27;as&#x27;, &#x27;at&#x27;,\n",
       "                                                                    &#x27;be&#x27;,\n",
       "                                                                    &#x27;because&#x27;,\n",
       "                                                                    &#x27;been&#x27;,\n",
       "                                                                    &#x27;before&#x27;,\n",
       "                                                                    &#x27;being&#x27;,\n",
       "                                                                    &#x27;below&#x27;,\n",
       "                                                                    &#x27;between&#x27;,\n",
       "                                                                    &#x27;both&#x27;,\n",
       "                                                                    &#x27;but&#x27;, &#x27;by&#x27;,\n",
       "                                                                    &#x27;can&#x27;,\n",
       "                                                                    &#x27;couldn&#x27;,\n",
       "                                                                    &quot;couldn&#x27;t&quot;, ...})),\n",
       "                                       (&#x27;clf&#x27;, LogisticRegression())]),\n",
       "             n_jobs=2,\n",
       "             param_grid={&#x27;tfidf__max_df&#x27;: (0.25, 0.5, 0.75),\n",
       "                         &#x27;tfidf__ngram_range&#x27;: [(1, 1)]},\n",
       "             verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=2,\n",
       "             estimator=Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                                        TfidfVectorizer(stop_words={&#x27;a&#x27;,\n",
       "                                                                    &#x27;about&#x27;,\n",
       "                                                                    &#x27;above&#x27;,\n",
       "                                                                    &#x27;after&#x27;,\n",
       "                                                                    &#x27;again&#x27;,\n",
       "                                                                    &#x27;against&#x27;,\n",
       "                                                                    &#x27;ain&#x27;,\n",
       "                                                                    &#x27;all&#x27;, &#x27;am&#x27;,\n",
       "                                                                    &#x27;an&#x27;, &#x27;and&#x27;,\n",
       "                                                                    &#x27;any&#x27;,\n",
       "                                                                    &#x27;are&#x27;,\n",
       "                                                                    &#x27;aren&#x27;,\n",
       "                                                                    &quot;aren&#x27;t&quot;,\n",
       "                                                                    &#x27;as&#x27;, &#x27;at&#x27;,\n",
       "                                                                    &#x27;be&#x27;,\n",
       "                                                                    &#x27;because&#x27;,\n",
       "                                                                    &#x27;been&#x27;,\n",
       "                                                                    &#x27;before&#x27;,\n",
       "                                                                    &#x27;being&#x27;,\n",
       "                                                                    &#x27;below&#x27;,\n",
       "                                                                    &#x27;between&#x27;,\n",
       "                                                                    &#x27;both&#x27;,\n",
       "                                                                    &#x27;but&#x27;, &#x27;by&#x27;,\n",
       "                                                                    &#x27;can&#x27;,\n",
       "                                                                    &#x27;couldn&#x27;,\n",
       "                                                                    &quot;couldn&#x27;t&quot;, ...})),\n",
       "                                       (&#x27;clf&#x27;, LogisticRegression())]),\n",
       "             n_jobs=2,\n",
       "             param_grid={&#x27;tfidf__max_df&#x27;: (0.25, 0.5, 0.75),\n",
       "                         &#x27;tfidf__ngram_range&#x27;: [(1, 1)]},\n",
       "             verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(stop_words={&#x27;a&#x27;, &#x27;about&#x27;, &#x27;above&#x27;, &#x27;after&#x27;,\n",
       "                                             &#x27;again&#x27;, &#x27;against&#x27;, &#x27;ain&#x27;, &#x27;all&#x27;,\n",
       "                                             &#x27;am&#x27;, &#x27;an&#x27;, &#x27;and&#x27;, &#x27;any&#x27;, &#x27;are&#x27;,\n",
       "                                             &#x27;aren&#x27;, &quot;aren&#x27;t&quot;, &#x27;as&#x27;, &#x27;at&#x27;, &#x27;be&#x27;,\n",
       "                                             &#x27;because&#x27;, &#x27;been&#x27;, &#x27;before&#x27;,\n",
       "                                             &#x27;being&#x27;, &#x27;below&#x27;, &#x27;between&#x27;,\n",
       "                                             &#x27;both&#x27;, &#x27;but&#x27;, &#x27;by&#x27;, &#x27;can&#x27;,\n",
       "                                             &#x27;couldn&#x27;, &quot;couldn&#x27;t&quot;, ...})),\n",
       "                (&#x27;clf&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words={&#x27;a&#x27;, &#x27;about&#x27;, &#x27;above&#x27;, &#x27;after&#x27;, &#x27;again&#x27;, &#x27;against&#x27;,\n",
       "                            &#x27;ain&#x27;, &#x27;all&#x27;, &#x27;am&#x27;, &#x27;an&#x27;, &#x27;and&#x27;, &#x27;any&#x27;, &#x27;are&#x27;,\n",
       "                            &#x27;aren&#x27;, &quot;aren&#x27;t&quot;, &#x27;as&#x27;, &#x27;at&#x27;, &#x27;be&#x27;, &#x27;because&#x27;,\n",
       "                            &#x27;been&#x27;, &#x27;before&#x27;, &#x27;being&#x27;, &#x27;below&#x27;, &#x27;between&#x27;,\n",
       "                            &#x27;both&#x27;, &#x27;but&#x27;, &#x27;by&#x27;, &#x27;can&#x27;, &#x27;couldn&#x27;, &quot;couldn&#x27;t&quot;, ...})</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=Pipeline(steps=[('tfidf',\n",
       "                                        TfidfVectorizer(stop_words={'a',\n",
       "                                                                    'about',\n",
       "                                                                    'above',\n",
       "                                                                    'after',\n",
       "                                                                    'again',\n",
       "                                                                    'against',\n",
       "                                                                    'ain',\n",
       "                                                                    'all', 'am',\n",
       "                                                                    'an', 'and',\n",
       "                                                                    'any',\n",
       "                                                                    'are',\n",
       "                                                                    'aren',\n",
       "                                                                    \"aren't\",\n",
       "                                                                    'as', 'at',\n",
       "                                                                    'be',\n",
       "                                                                    'because',\n",
       "                                                                    'been',\n",
       "                                                                    'before',\n",
       "                                                                    'being',\n",
       "                                                                    'below',\n",
       "                                                                    'between',\n",
       "                                                                    'both',\n",
       "                                                                    'but', 'by',\n",
       "                                                                    'can',\n",
       "                                                                    'couldn',\n",
       "                                                                    \"couldn't\", ...})),\n",
       "                                       ('clf', LogisticRegression())]),\n",
       "             n_jobs=2,\n",
       "             param_grid={'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                         'tfidf__ngram_range': [(1, 1)]},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tune = GridSearchCV(\n",
    "    pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
    "grid_search_tune.fit(train_X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b4f7a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "{'tfidf__max_df': 0.25, 'tfidf__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8ebd5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.98       958\n",
      "        spam       0.99      0.79      0.88       157\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.89      0.93      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "best_clf = grid_search_tune.best_estimator_\n",
    "predictions = best_clf.predict(test_X)\n",
    "\n",
    "print(metrics.classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57968ac",
   "metadata": {},
   "source": [
    "# Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adfff16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.predict([\"Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f148c041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.predict([\"Reply to claim your reward of $20,000\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f87603f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.predict([\"This is to inform you that you have a meeting scheduled today at 4:00 pm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b17ab637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf.predict([\"Congratulations on successfully completing an online course on Machine Learning Application offered by Great Learning Academy.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa154a14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
